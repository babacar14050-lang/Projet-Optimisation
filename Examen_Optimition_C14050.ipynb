{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed0e842",
   "metadata": {},
   "source": [
    "PROJET EXAMEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fa195",
   "metadata": {},
   "source": [
    "Ce projet vise à illustrer les liens entre la théorie de l'optimisation convexe et les algorithmes\n",
    "modernes utilisés en Machine Learning à grande échelle. Nous étudions successivement la mod\n",
    "élisation, les méthodes de gradient déterministes et stochastiques, puis l'optimisation non lisse\n",
    "via des algorithmes proximaux.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e1eb8",
   "metadata": {},
   "source": [
    " preparer par DIENG BABAKAR C14050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47485439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des bibliothéque necessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_rcv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50e7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         Year  TimbreAvg1  TimbreAvg2  TimbreAvg3  TimbreAvg4  TimbreAvg5  \\\n",
       "0       2001    49.94357    21.47114    73.07750     8.74861   -17.40628   \n",
       "1       2001    48.73215    18.42930    70.32679    12.94636   -10.32437   \n",
       "2       2001    50.95714    31.85602    55.81851    13.41693    -6.57898   \n",
       "3       2001    48.24750    -1.89837    36.29772     2.58776     0.97170   \n",
       "4       2001    50.97020    42.20998    67.09964     8.46791   -15.85279   \n",
       "...      ...         ...         ...         ...         ...         ...   \n",
       "515340  2006    51.28467    45.88068    22.19582    -5.53319    -3.61835   \n",
       "515341  2006    49.87870    37.93125    18.65987    -3.63581   -27.75665   \n",
       "515342  2006    45.12852    12.65758   -38.72018     8.80882   -29.29985   \n",
       "515343  2006    44.16614    32.38368    -3.34971    -2.49165   -19.59278   \n",
       "515344  2005    51.85726    59.11655    26.39436    -5.46030   -20.69012   \n",
       "\n",
       "        TimbreAvg6  TimbreAvg7  TimbreAvg8  TimbreAvg9  ...  \\\n",
       "0        -13.09905   -25.01202   -12.23257     7.83089  ...   \n",
       "1        -24.83777     8.76630    -0.92019    18.76548  ...   \n",
       "2        -18.54940    -3.27872    -2.35035    16.07017  ...   \n",
       "3        -26.21683     5.05097   -10.34124     3.55005  ...   \n",
       "4        -16.81409   -12.48207    -9.37636    12.63699  ...   \n",
       "...            ...         ...         ...         ...  ...   \n",
       "515340   -16.36914     2.12652     5.18160    -8.66890  ...   \n",
       "515341   -18.52988     7.76108     3.56109    -2.50351  ...   \n",
       "515342    -2.28706   -18.40424   -22.28726    -4.52429  ...   \n",
       "515343   -18.67098     8.78428     4.02039   -12.01230  ...   \n",
       "515344   -19.95528    -6.72771     2.29590    10.31018  ...   \n",
       "\n",
       "        TimbreCovariance69  TimbreCovariance70  TimbreCovariance71  \\\n",
       "0                 13.01620           -54.40548            58.99367   \n",
       "1                  5.66812           -19.68073            33.04964   \n",
       "2                  3.03800            26.05866           -50.92779   \n",
       "3                 34.57337          -171.70734           -16.96705   \n",
       "4                  9.92661           -55.95724            64.92712   \n",
       "...                    ...                 ...                 ...   \n",
       "515340             4.81440            -3.75991           -30.92584   \n",
       "515341            32.38589           -32.75535           -61.05473   \n",
       "515342           -18.73598           -71.15954          -123.98443   \n",
       "515343            67.16763           282.77624            -4.63677   \n",
       "515344           -11.50511           -69.18291            60.58456   \n",
       "\n",
       "        TimbreCovariance72  TimbreCovariance73  TimbreCovariance74  \\\n",
       "0                 15.37344             1.11144           -23.08793   \n",
       "1                 42.87836            -9.90378           -32.22788   \n",
       "2                 10.93792            -0.07568            43.20130   \n",
       "3                -46.67617           -12.51516            82.58061   \n",
       "4                -17.72522            -1.49237            -7.50035   \n",
       "...                    ...                 ...                 ...   \n",
       "515340            26.33968            -5.03390            21.86037   \n",
       "515341            56.65182            15.29965            95.88193   \n",
       "515342           121.26989            10.89629            34.62409   \n",
       "515343           144.00125            21.62652           -29.72432   \n",
       "515344            28.64599            -4.39620           -64.56491   \n",
       "\n",
       "        TimbreCovariance75  TimbreCovariance76  TimbreCovariance77  \\\n",
       "0                 68.40795            -1.82223           -27.46348   \n",
       "1                 70.49388            12.04941            58.43453   \n",
       "2               -115.00698            -0.05859            39.67068   \n",
       "3                -72.08993             9.90558           199.62971   \n",
       "4                 51.76631             7.88713            55.66926   \n",
       "...                    ...                 ...                 ...   \n",
       "515340          -142.29410             3.42901           -41.14721   \n",
       "515341           -10.63242            12.96552            92.11633   \n",
       "515342          -248.61020            -6.07171            53.96319   \n",
       "515343            71.47198            20.32240            14.83107   \n",
       "515344           -45.61012            -5.51512            32.35602   \n",
       "\n",
       "        TimbreCovariance78  \n",
       "0                  2.26327  \n",
       "1                 26.92061  \n",
       "2                 -0.66345  \n",
       "3                 18.85382  \n",
       "4                 28.74903  \n",
       "...                    ...  \n",
       "515340           -15.46052  \n",
       "515341            10.88815  \n",
       "515342            -8.09364  \n",
       "515343            39.74909  \n",
       "515344            12.17352  \n",
       "\n",
       "[515345 rows x 91 columns]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('YearPredictionMSD.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99906d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Échantillon réduit : n = 5000 d = 90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# EXERCICE 1 — YearPredictionMSD (Régression)\n",
    "# Séparer X et y\n",
    "y = df.iloc[:, 0].values    # La première colonne : année\n",
    "X = df.iloc[:, 1:].values   # Les colonnes suivantes : features\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Sous-échantillon pour tests rapides\n",
    "X_sub, _, y_sub, _ = train_test_split(X, y, train_size=5000, random_state=0)\n",
    "n, d = X_sub.shape\n",
    "print(\"Échantillon réduit : n =\", n, \"d =\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112e1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*, block=None)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------\n",
    "# MSE et gradient\n",
    "# -------------------\n",
    "def mse_loss(w, X, y):\n",
    "    return 0.5 * np.mean((X @ w - y) ** 2)\n",
    "\n",
    "def mse_gradient(w, X, y):\n",
    "    return (X.T @ (X @ w - y)) / X.shape[0]\n",
    "\n",
    "# -------------------\n",
    "# Optimisateurs\n",
    "# -------------------\n",
    "def batch_gd(X, y, alpha, n_iter):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    losses, times = [], []\n",
    "    start = time.time()\n",
    "    for _ in range(n_iter):\n",
    "        w -= alpha * mse_gradient(w, X, y)\n",
    "        losses.append(mse_loss(w, X, y))\n",
    "        times.append(time.time() - start)\n",
    "    return w, losses, times\n",
    "\n",
    "def sgd(X, y, alpha0, n_iter):\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    losses, times = [], []\n",
    "    start = time.time()\n",
    "    for k in range(n_iter):\n",
    "        i = np.random.randint(n)\n",
    "        grad = X[i] * (X[i] @ w - y[i])\n",
    "        w -= (alpha0 / (1 + k)) * grad\n",
    "        losses.append(mse_loss(w, X, y))\n",
    "        times.append(time.time() - start)\n",
    "    return w, losses, times\n",
    "\n",
    "def minibatch_sgd(X, y, alpha, batch_size, n_iter):\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    losses, times = [], []\n",
    "    start = time.time()\n",
    "    for _ in range(n_iter):\n",
    "        idx = np.random.choice(n, batch_size, replace=False)\n",
    "        grad = (X[idx].T @ (X[idx] @ w - y[idx])) / batch_size\n",
    "        w -= alpha * grad\n",
    "        losses.append(mse_loss(w, X, y))\n",
    "        times.append(time.time() - start)\n",
    "    return w, losses, times\n",
    "\n",
    "def adam(X, y, alpha, beta1, beta2, eps, n_iter):\n",
    "    d = X.shape[1]\n",
    "    w = np.zeros(d)\n",
    "    m = np.zeros(d)\n",
    "    v = np.zeros(d)\n",
    "    losses, times = [], []\n",
    "    start = time.time()\n",
    "    for k in range(1, n_iter + 1):\n",
    "        grad = mse_gradient(w, X, y)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "        m_hat = m / (1 - beta1**k)\n",
    "        v_hat = v / (1 - beta2**k)\n",
    "        w -= alpha * m_hat / (np.sqrt(v_hat) + eps)\n",
    "        losses.append(mse_loss(w, X, y))\n",
    "        times.append(time.time() - start)\n",
    "    return w, losses, times\n",
    "\n",
    "# -------------------\n",
    "# Comparaison\n",
    "# -------------------\n",
    "w_bg, l_bg, t_bg = batch_gd(X_sub, y_sub, 1e-2, 200)\n",
    "w_sgd, l_sgd, t_sgd = sgd(X_sub, y_sub, 0.1, 300)\n",
    "w_mb, l_mb, t_mb = minibatch_sgd(X_sub, y_sub, 0.01, 256, 300)\n",
    "w_adam, l_adam, t_adam = adam(X_sub, y_sub, 0.05, 0.9, 0.999, 1e-8, 300)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(t_bg, l_bg, label=\"Batch GD\")\n",
    "plt.plot(t_sgd, l_sgd, label=\"SGD\")\n",
    "plt.plot(t_mb, l_mb, label=\"Mini-batch\")\n",
    "plt.plot(t_adam, l_adam, label=\"Adam\")\n",
    "plt.xlabel(\"Temps CPU (s)\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Convergence — YearPredictionMSD\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"convergence_YearPredictionMSD.png\")\n",
    "# plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed055978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement RCV1...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# EXERCICE 3 — RCV1 (Classification L1)\n",
    "# ===============================\n",
    "print(\"Chargement RCV1...\")\n",
    "X, y = fetch_rcv1(return_X_y=True)\n",
    "\n",
    "# Binarisation pour la première catégorie\n",
    "y = np.where(y[:,0].toarray() > 0, 1, -1)\n",
    "\n",
    "# Sous-échantillon pour tests rapides\n",
    "X = X[:5000].toarray()\n",
    "y = y[:5000].ravel()\n",
    "\n",
    "# Standardisation\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# -------------------\n",
    "# Logistic loss et gradient\n",
    "# -------------------\n",
    "def logistic_loss(w, X, y):\n",
    "    z = y * (X @ w)\n",
    "    return np.mean(np.log(1 + np.exp(-z)))\n",
    "\n",
    "def logistic_gradient(w, X, y):\n",
    "    z = y * (X @ w)\n",
    "    sigma = 1 / (1 + np.exp(z))\n",
    "    return -(X.T @ (y * sigma)) / X.shape[0]\n",
    "\n",
    "# -------------------\n",
    "# ISTA / FISTA\n",
    "# -------------------\n",
    "def soft_thresholding(v, lam):\n",
    "    return np.sign(v) * np.maximum(np.abs(v) - lam, 0)\n",
    "\n",
    "def ista(X, y, lam, alpha, n_iter):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    for _ in range(n_iter):\n",
    "        w = soft_thresholding(w - alpha * logistic_gradient(w, X, y), alpha * lam)\n",
    "        losses.append(logistic_loss(w, X, y) + lam * np.linalg.norm(w, 1))\n",
    "    return w, losses\n",
    "\n",
    "def fista(X, y, lam, alpha, n_iter):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    z = w.copy()\n",
    "    t = 1\n",
    "    losses = []\n",
    "    for _ in range(n_iter):\n",
    "        w_new = soft_thresholding(z - alpha * logistic_gradient(z, X, y), alpha * lam)\n",
    "        t_new = (1 + np.sqrt(1 + 4 * t**2)) / 2\n",
    "        z = w_new + ((t - 1) / t_new) * (w_new - w)\n",
    "        w, t = w_new, t_new\n",
    "        losses.append(logistic_loss(w, X, y) + lam * np.linalg.norm(w, 1))\n",
    "    return w, losses\n",
    "\n",
    "# -------------------\n",
    "# Sparsité vs λ\n",
    "# -------------------\n",
    "lambdas = np.logspace(-4, -1, 6)\n",
    "zeros_ista, zeros_fista = [], []\n",
    "\n",
    "for lam in lambdas:\n",
    "    w_i, _ = ista(X, y, lam, 0.1, 100)\n",
    "    w_f, _ = fista(X, y, lam, 0.1, 100)\n",
    "    zeros_ista.append(np.sum(np.abs(w_i) < 1e-6))\n",
    "    zeros_fista.append(np.sum(np.abs(w_f) < 1e-6))\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(lambdas, zeros_ista, marker=\"o\", label=\"ISTA\")\n",
    "plt.semilogx(lambdas, zeros_fista, marker=\"s\", label=\"FISTA\")\n",
    "plt.xlabel(\"λ\")\n",
    "plt.ylabel(\"Nombre de coefficients nuls\")\n",
    "plt.title(\"Sélection de variables — RCV1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"sparsity_RCV1.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
